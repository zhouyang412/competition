{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cleared-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "yellow-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    def __init__(self,\n",
    "                 set_type,\n",
    "                 text,\n",
    "                 id_=None,\n",
    "                 label=None):\n",
    "        self.set_type = set_type\n",
    "        self.id_ = id_\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        \n",
    "class BaseFeature:\n",
    "    def __init__(self,\n",
    "                 token_ids,\n",
    "                 attention_masks,\n",
    "                 token_type_ids):\n",
    "        # BERT 输入\n",
    "        self.token_ids = token_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.token_type_ids = token_type_ids\n",
    "        \n",
    "class BertFeature(BaseFeature):\n",
    "    def __init__(self,\n",
    "                 token_ids,\n",
    "                 attention_masks,\n",
    "                 token_type_ids,\n",
    "                 id_=None,\n",
    "                 label=None):\n",
    "        super(BertFeature, self).__init__(token_ids=token_ids,\n",
    "                                         attention_masks=attention_masks,\n",
    "                                         token_type_ids=token_type_ids)\n",
    "        self.label = label\n",
    "        self.id_ = id_\n",
    "\n",
    "        \n",
    "class Processor:\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(file_path):\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_examples(file_path):\n",
    "        df = Processor.read_data(file_path)\n",
    "        examples = []\n",
    "        df_columns = df.columns.tolist()\n",
    "        \n",
    "        if 'label' in df_columns:\n",
    "            set_type = 'train'\n",
    "        else:\n",
    "            set_type = 'test'\n",
    "        \n",
    "        if set_type == 'train':\n",
    "            for idx, (label, text) in enumerate(zip(df.label, df.comment)):\n",
    "                examples.append(InputExample(set_type=set_type,\n",
    "                                             text=text,\n",
    "                                             label=label))\n",
    "        else:\n",
    "            for idx, (id_, text) in enumerate(zip(df.id, df.comment)):\n",
    "                label = None\n",
    "                examples.append(InputExample(set_type=set_type,\n",
    "                                             id_=id_,\n",
    "                                             text=text,\n",
    "                                             label=label))            \n",
    "            \n",
    "        return examples\n",
    "    \n",
    "def convert_input_example(example: InputExample, tokenizer: BertTokenizer,\n",
    "                        max_seq_len):\n",
    "    set_type = example.set_type\n",
    "    id_ = example.id_\n",
    "    text = example.text\n",
    "    label = example.label\n",
    "\n",
    "    encode_dict = tokenizer.encode_plus(text=text,\n",
    "                                        max_length=max_seq_len,\n",
    "                                        pad_to_max_length=True,\n",
    "                                        return_token_type_ids=True,\n",
    "                                        return_attention_mask=True,\n",
    "                                        truncation=True,\n",
    "                                        padding=True)\n",
    "\n",
    "\n",
    "    token_ids = encode_dict['input_ids']\n",
    "    attention_masks = encode_dict['attention_mask']\n",
    "    token_type_ids = encode_dict['token_type_ids']\n",
    "\n",
    "    out_len = len(encode_dict['input_ids'])\n",
    "    pad_len = max_seq_len - out_len\n",
    "\n",
    "    token_ids  = encode_dict['input_ids'] + [0] * pad_len\n",
    "    attention_masks  = encode_dict['attention_mask'] + [0] * pad_len\n",
    "    token_type_ids  = encode_dict['token_type_ids'] + [0] * pad_len\n",
    "\n",
    "    feature = BertFeature(\n",
    "        # bert inputs\n",
    "        token_ids=token_ids,\n",
    "        attention_masks=attention_masks,\n",
    "        token_type_ids=token_type_ids,\n",
    "        id_=id_,\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "    return feature\n",
    "\n",
    "def convert_examples_to_features(examples, max_seq_len, bert_path):\n",
    "\n",
    "    tokenizer = BertTokenizer(os.path.join(bert_path, 'vocab.txt'))\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for i, example in enumerate(examples):\n",
    "        \n",
    "        feature = convert_input_example(\n",
    "            example=example,\n",
    "            max_seq_len=max_seq_len,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        if feature is None:\n",
    "            continue\n",
    "\n",
    "        features.append(feature)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'\n",
    "bert_path = '/Users/zy/bert_wwm_ext/'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "train_examples = Processor.get_examples(train_path)\n",
    "train_features = convert_examples_to_features(examples=train_examples, max_seq_len=156, bert_path=bert_path)\n",
    "\n",
    "test_examples = Processor.get_examples(test_path)\n",
    "test_features = convert_examples_to_features(examples=test_examples, max_seq_len=156, bert_path=bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "utility-settle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0011f384-9e54-4fb4-a272-330a6cab6804'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features[0].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "guilty-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_utils import ComDataset\n",
    "import torch \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "immediate-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ComDataset(train_features, 'train')\n",
    "test_dataset = ComDataset(test_features, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "existing-poster",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ComDataset' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-108da99aabc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'ComDataset' object is not an iterator"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
