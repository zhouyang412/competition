{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fossil-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "popular-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    设置随机种子\n",
    "    :param seed:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    random.seed(seed) # Python\n",
    "    np.random.seed(seed) # cpu vars\n",
    "    torch.manual_seed(seed) # cpu  vars\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "automotive-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    def __init__(self,\n",
    "                 set_type,\n",
    "                 text,\n",
    "                 id_=None,\n",
    "                 label=None):\n",
    "        self.set_type = set_type\n",
    "        self.id_ = id_\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        \n",
    "class BaseFeature:\n",
    "    def __init__(self,\n",
    "                 token_ids,\n",
    "                 attention_masks,\n",
    "                 token_type_ids):\n",
    "        # BERT 输入\n",
    "        self.token_ids = token_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.token_type_ids = token_type_ids\n",
    "        \n",
    "class BertFeature(BaseFeature):\n",
    "    def __init__(self,\n",
    "                 token_ids,\n",
    "                 attention_masks,\n",
    "                 token_type_ids,\n",
    "                 id_=None,\n",
    "                 label=None):\n",
    "        super(BertFeature, self).__init__(token_ids=token_ids,\n",
    "                                         attention_masks=attention_masks,\n",
    "                                         token_type_ids=token_type_ids)\n",
    "        self.label = label\n",
    "        self.id_ = id_\n",
    "\n",
    "        \n",
    "class Processor:\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(file_path):\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_examples(file_path):\n",
    "        df = Processor.read_data(file_path)\n",
    "        examples = []\n",
    "        df_columns = df.columns.tolist()\n",
    "        \n",
    "        if 'label' in df_columns:\n",
    "            set_type = 'train'\n",
    "        else:\n",
    "            set_type = 'test'\n",
    "        \n",
    "        if set_type == 'train':\n",
    "            for idx, (label, text) in enumerate(zip(df.label, df.comment)):\n",
    "                examples.append(InputExample(set_type=set_type,\n",
    "                                             text=text,\n",
    "                                             label=label))\n",
    "        else:\n",
    "            for idx, (id_, text) in enumerate(zip(df.id, df.comment)):\n",
    "                label = None\n",
    "                examples.append(InputExample(set_type=set_type,\n",
    "                                             id_=id_,\n",
    "                                             text=text,\n",
    "                                             label=label))            \n",
    "            \n",
    "        return examples\n",
    "    \n",
    "def convert_input_example(example: InputExample, tokenizer: BertTokenizer,\n",
    "                        max_seq_len):\n",
    "    set_type = example.set_type\n",
    "    id_ = example.id_\n",
    "    text = example.text\n",
    "    label = example.label\n",
    "\n",
    "    encode_dict = tokenizer.encode_plus(text=text,\n",
    "                                        max_length=max_seq_len,\n",
    "                                        pad_to_max_length=True,\n",
    "                                        return_token_type_ids=True,\n",
    "                                        return_attention_mask=True,\n",
    "                                        truncation=True,\n",
    "                                        padding=True)\n",
    "\n",
    "\n",
    "    token_ids = encode_dict['input_ids']\n",
    "    attention_masks = encode_dict['attention_mask']\n",
    "    token_type_ids = encode_dict['token_type_ids']\n",
    "\n",
    "    out_len = len(encode_dict['input_ids'])\n",
    "    pad_len = max_seq_len - out_len\n",
    "\n",
    "    token_ids  = encode_dict['input_ids'] + [0] * pad_len\n",
    "    attention_masks  = encode_dict['attention_mask'] + [0] * pad_len\n",
    "    token_type_ids  = encode_dict['token_type_ids'] + [0] * pad_len\n",
    "\n",
    "    feature = BertFeature(\n",
    "        # bert inputs\n",
    "        token_ids=token_ids,\n",
    "        attention_masks=attention_masks,\n",
    "        token_type_ids=token_type_ids,\n",
    "        id_=id_,\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "    return feature\n",
    "\n",
    "def convert_examples_to_features(examples, max_seq_len, bert_path):\n",
    "\n",
    "    tokenizer = BertTokenizer(os.path.join(bert_path, 'vocab.txt'))\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for i, example in enumerate(examples):\n",
    "        \n",
    "        feature = convert_input_example(\n",
    "            example=example,\n",
    "            max_seq_len=max_seq_len,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        if feature is None:\n",
    "            continue\n",
    "\n",
    "        features.append(feature)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'\n",
    "bert_path = '/Users/zy/bert_wwm_ext/'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "train_examples = Processor.get_examples(train_path)\n",
    "train_features = convert_examples_to_features(examples=train_examples, max_seq_len=156, bert_path=bert_path)\n",
    "\n",
    "test_examples = Processor.get_examples(test_path)\n",
    "test_features = convert_examples_to_features(examples=test_examples, max_seq_len=156, bert_path=bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "modular-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    \n",
    "    train_path = './data/train.csv'\n",
    "    test_path = './data/test.csv'\n",
    "    sample_path = './data/sample.csv'\n",
    "    bert_path = '/Users/zy/bert_wwm_ext/'\n",
    "    \n",
    "    hid_size = 768\n",
    "    seed = 666\n",
    "    train_epochs = 1\n",
    "    train_batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    bert_lr = 1e-5\n",
    "    classifier_lr = 1e-4\n",
    "    adam_epsilon = 1e-12\n",
    "    warmup_proportion = 1\n",
    "    weight_decay = 0.01\n",
    "    attack_train_mode = None\n",
    "    max_grad_norm = 5\n",
    "    \n",
    "    attack_train = None\n",
    "    loss_fn = 'ce'\n",
    "    accumulation_steps = 1\n",
    "    \n",
    "    model_save_path = './best_models/'\n",
    "    model_save_name = 'baseline'\n",
    "    \n",
    "    use_gpu = False\n",
    "    mul_gpu = False\n",
    "    \n",
    "    \n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "buried-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_utils import ComDataset\n",
    "from models.layers import MaskedGlobalMaxPool1D, MaskedGlobalAveragePooling1D\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dated-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ComDataset(train_features, 'train')\n",
    "test_dataset = ComDataset(test_features, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "identical-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=args.train_batch_size,\n",
    "                          sampler=train_sampler)\n",
    "\n",
    "dev_loader = train_loader\n",
    "\n",
    "# test_sampler = RandomSampler(test_dataset)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=args.test_batch_size,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "solved-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['token_ids', 'attention_masks', 'token_type_ids', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i.keys())\n",
    "    token_ids = i['token_ids']\n",
    "    attention_masks = i['attention_masks']\n",
    "    token_type_ids = i['token_type_ids']\n",
    "    labels = i['labels']\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "resistant-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, bert_path):\n",
    "        super(BaseModel, self).__init__()\n",
    "        config_path = os.path.join(bert_path, 'config.json')\n",
    "\n",
    "        assert os.path.exists(bert_path) and os.path.exists(config_path), 'pretrained bert file does not exist'\n",
    "        #'/home/dc2-user/p_data/nlp_pretrained/bert_chinese/chinese_roberta_wwm_large_ext'\n",
    "        self.bert_module = BertModel.from_pretrained(bert_path, output_hidden_states=True)\n",
    "\n",
    "        self.bert_config = self.bert_module.config\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(blocks, **kwargs):\n",
    "        \"\"\"\n",
    "        参数初始化，将 Linear / Embedding / LayerNorm 与 Bert 进行一样的初始化\n",
    "        \"\"\"\n",
    "        for block in blocks:\n",
    "            for module in block.modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "                elif isinstance(module, nn.Embedding):\n",
    "                    nn.init.normal_(module.weight, mean=0, std=kwargs.pop('initializer_range', 0.02))\n",
    "                elif isinstance(module, nn.LayerNorm):\n",
    "                    nn.init.ones_(module.weight)\n",
    "                    nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "front-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLineClassifier(nn.Module):\n",
    "    def __init__(self, args, n_class=2):\n",
    "        super(BaseLineClassifier, self).__init__()\n",
    "        \n",
    "        hid_size = args.hid_size\n",
    "        self.avg_pooling = MaskedGlobalAveragePooling1D()\n",
    "        self.max_pooling = MaskedGlobalMaxPool1D()\n",
    "        self.dropout = nn.Dropout(0.2)#args.mt_dropout_prob[0]\n",
    "        self.dropouts = nn.ModuleList([\n",
    "            nn.Dropout(0.5) for _ in range(5)\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.Linear(hid_size * 5, n_class)\n",
    "\n",
    "    def forward(self, all_hidden_states, pooler_output, input_mask):\n",
    "        last_hidden_states = all_hidden_states[-2:]\n",
    "        last_hidden_state = torch.cat((last_hidden_states[-1], last_hidden_states[-2]), dim=2)\n",
    "        avg_pooled = self.avg_pooling(last_hidden_state, input_mask)\n",
    "        max_pooled = self.max_pooling(last_hidden_state, input_mask)\n",
    "        pooled = torch.cat((avg_pooled, max_pooled, pooler_output), dim=1)\n",
    "\n",
    "\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                h = self.classifier(dropout(pooled))\n",
    "            else:\n",
    "                h += self.classifier(dropout(pooled))\n",
    "        h = h / len(self.dropouts)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "guided-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForWardModel(BaseModel):\n",
    "    def __init__(self, args, classifier=None):\n",
    "        super(ForWardModel, self).__init__(args.bert_path)\n",
    "        assert classifier is not None, '检查分类器是否正常输入。'\n",
    "        \n",
    "        self.classifier = classifier\n",
    "            \n",
    "    def forward(self, token_ids, attention_masks, token_type_ids):\n",
    "        \n",
    "        all_bert_outputs = self.bert_module(input_ids=token_ids,\n",
    "                                            attention_mask=attention_masks,\n",
    "                                            token_type_ids=token_type_ids)\n",
    "#         print(all_bert_outputs)\n",
    "        last_hidden_state = all_bert_outputs.last_hidden_state\n",
    "        pooler_output = all_bert_outputs.pooler_output\n",
    "        all_hidden_states = all_bert_outputs.hidden_states\n",
    "        \n",
    "        outputs = self.classifier(all_hidden_states, pooler_output, attention_masks)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "classifier = BaseLineClassifier(args)\n",
    "model = ForWardModel(args, classifier)\n",
    "h = model(token_ids, attention_masks, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abandoned-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from utils.attack_train_utils import FGM, PGD\n",
    "from models.losses import FocalLoss, LabelSmoothingCrossEntropy\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import math\n",
    "t_total=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "julian-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer_scheduler(model, t_total):\n",
    "    module = (model.module if hasattr(model, \"module\") else model)\n",
    "    # 差分学习率\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    embedding_param = list(module.bert_module.named_parameters())\n",
    "    classifier_param = list(module.classifier.named_parameters())\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        # bert other module\n",
    "        {\"params\": [p for n, p in embedding_param if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": args.weight_decay, 'lr': args.bert_lr},\n",
    "        {\"params\": [p for n, p in embedding_param if any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0, 'lr': args.bert_lr},\n",
    "\n",
    "        # other\n",
    "        {\"params\": [p for n, p in classifier_param if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": args.weight_decay, 'lr': args.classifier_lr},\n",
    "        {\"params\": [p for n, p in classifier_param if any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0, 'lr': args.classifier_lr},\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.bert_lr, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=math.ceil(args.warmup_proportion * t_total), \n",
    "        num_training_steps=t_total\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "textile-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    \n",
    "    if len(y_pred.shape)>1:\n",
    "        y_pred = np.argmax(y_pred,axis=1)\n",
    "    if len(y_true.shape)>1:\n",
    "        y_true = np.argmax(y_true,axis=1)\n",
    "        \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true,y_pred,average= 'macro')\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tender-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args model train_loader\n",
    "\n",
    "def train_model(args, model, train_loader):\n",
    "    \n",
    "    loss_name = args.loss_fn.lower()\n",
    "    assert loss_name in ['ce', 'focalloss', 'labelsmooth'], '请输入正确的lossfn，ex: ce, focalloss, labelsmooth'\n",
    "\n",
    "    if loss_name == 'ce':\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    elif loss_name == 'focalloss':\n",
    "        loss_fn = FocalLoss()\n",
    "    elif loss_name == 'labelsmooth':\n",
    "        loss_fn = LabelSmoothingCrossEntropy()\n",
    "\n",
    "    if args.use_gpu:\n",
    "        device = torch.device('gpu')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    train_f1 = []\n",
    "    train_acc = []\n",
    "    one_epoch_steps = len(train_loader)\n",
    "    optimizer, scheduler = build_optimizer_scheduler(model, math.ceil(one_epoch_steps / args.accumulation_steps))\n",
    "\n",
    "    global_step = 0\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    fgm, pgd = None, None\n",
    "\n",
    "    attack_train_mode = args.attack_train\n",
    "    if attack_train_mode is not None:\n",
    "        attack_train_mode = attack_train_mode.lower()\n",
    "    if attack_train_mode == 'fgm':\n",
    "        fgm = FGM(model=model)\n",
    "    elif attack_train_mode == 'pgd':\n",
    "        pgd = PGD(model=model)\n",
    "\n",
    "\n",
    "    for epoch in range(args.train_epochs):\n",
    "        tk0 = tqdm(train_loader, total=one_epoch_steps)\n",
    "        for step, batch_data in enumerate(tk0):\n",
    "            token_ids = batch_data['token_ids'].to(device)\n",
    "            attention_masks = batch_data['attention_masks'].to(device)\n",
    "            token_type_ids = batch_data['token_type_ids'].to(device)\n",
    "            labels = batch_data['labels'].to(device)\n",
    "\n",
    "            outputs = model(token_ids=token_ids, \n",
    "                            attention_masks=attention_masks, \n",
    "                            token_type_ids=token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss = loss.mean() / args.accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if fgm is not None:\n",
    "                fgm.attack()\n",
    "                outputs = model(token_ids=token_ids, \n",
    "                                attention_masks=attention_masks, \n",
    "                                token_type_ids=token_type_ids)\n",
    "                loss_adv = loss_fn(outputs, labels)\n",
    "                loss_adv = loss_adv.mean() / args.accumulation_steps\n",
    "                loss_adv.backward()\n",
    "                fgm.restore()\n",
    "\n",
    "            elif pgd is not None: # 梯度累计pgd可能会有问题\n",
    "                pgd.backup_grad()\n",
    "\n",
    "                for _t in range(pgd_k):\n",
    "                    pgd.attack(is_first_attack=(_t == 0))\n",
    "                    if _t != pgd_k - 1:\n",
    "                        model.zero_grad()\n",
    "                    else:\n",
    "                        pgd.restore_grad()\n",
    "                    outputs = model(token_ids=token_ids, \n",
    "                                    attention_masks=attention_masks, \n",
    "                                    token_type_ids=token_type_ids)\n",
    "                    loss_adv = loss_fn(outputs, labels)\n",
    "                    loss_adv = loss_adv.mean()\n",
    "                    loss_adv.backward()\n",
    "                pgd.restore()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm) \n",
    "\n",
    "            if (step + 1) % args.accumulation_steps == 0 or (step + 1) == one_epoch_steps + 1:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    model.zero_grad()\n",
    "\n",
    "            labels = labels.cpu().detach().numpy() \n",
    "            outputs = outputs.cpu().detach().numpy()\n",
    "            acc, f1 = metric(labels, outputs)\n",
    "            train_f1.append(f1)\n",
    "            train_acc.append(acc)\n",
    "            torch.cuda.empty_cache()\n",
    "            tk0.set_postfix(train_f1=np.mean(train_f1), \n",
    "                            train_acc=np.mean(train_acc))         \n",
    "\n",
    "            if step > 10:\n",
    "                break\n",
    "\n",
    "    # swa(swa_raw_model, args.output_dir, swa_start=args.swa_start)\n",
    "\n",
    "    # clear cuda cache to avoid OOM\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    logger.info('Train done')        \n",
    "        \n",
    "    \n",
    "# train(args, model, train_loader)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secondary-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(args, model, best_f1):\n",
    "    \n",
    "    if args.use_gpu:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        model.eval()\n",
    "    dev_f1 = []\n",
    "    dev_acc = []\n",
    "    one_epoch_steps = len(dev_loader)\n",
    "\n",
    "    tk0 = tqdm(dev_loader, total=one_epoch_steps)\n",
    "    for step, batch_data in enumerate(tk0):\n",
    "        token_ids = batch_data['token_ids'].to(device)\n",
    "        attention_masks = batch_data['attention_masks'].to(device)\n",
    "        token_type_ids = batch_data['token_type_ids'].to(device)\n",
    "        labels = batch_data['labels'].to(device)\n",
    "\n",
    "        outputs = model(token_ids=token_ids, \n",
    "                        attention_masks=attention_masks, \n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        labels = labels.cpu().detach().numpy() \n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        acc, f1 = metric(labels, outputs)\n",
    "        train_f1.append(f1)\n",
    "        train_acc.append(acc)\n",
    "        torch.cuda.empty_cache()\n",
    "        if step>10:\n",
    "            break\n",
    "        tk0.set_postfix(train_f1=np.mean(train_f1), \n",
    "                        train_acc=np.mean(train_acc))  \n",
    "    \n",
    "    if np.mean(dev_f1) > best_f1:\n",
    "        model_save_allname = args.model_save_name + '_seed-' + str(args.seed) + '_bs-' + str(args.train_batch_size)\n",
    "        model_save_dir = args.model_save_path + model_save_allname + '.bin'\n",
    "        torch.save(model.state_dict(), model_save_dir)\n",
    "        \n",
    "        \n",
    "# eval_model(model, best_f1=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nominated-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/500 [00:17<12:38,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "def test_model(args, model, test_loader):\n",
    "    if args.use_gpu:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        model.eval()\n",
    "    model.eval()\n",
    "    one_epoch_steps = len(test_loader)\n",
    "    test_preds = []\n",
    "    tk0 = tqdm(test_loader, total=one_epoch_steps)\n",
    "    for step, batch_data in enumerate(tk0):\n",
    "        token_ids = batch_data['token_ids'].to(device)\n",
    "        attention_masks = batch_data['attention_masks'].to(device)\n",
    "        token_type_ids = batch_data['token_type_ids'].to(device)\n",
    "\n",
    "        logits = model(token_ids=token_ids, \n",
    "                        attention_masks=attention_masks, \n",
    "                        token_type_ids=token_type_ids)\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().detach().numpy() \n",
    "        test_preds.append(preds)\n",
    "        test_preds = np.hstack(test_preds)\n",
    "    return test_preds\n",
    "\n",
    "# test_preds = test_model(args, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "desperate-florida",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack(test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
